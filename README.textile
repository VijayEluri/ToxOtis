h1. ToxOtis

Version: 0.0, planning

ToxOtis is the Greek word for Sagittarius, that actually means 'archer'. ToxOtis is more than a web client and parser; it is a versatile tool for using the OpenTox predictive toxicology web services from inside your Java application providing a flexible API. 

h2. 1. Components

h3. 1.1. Necessary Components

The following components should be part of the OpenTox client:
* Compound/Conformer (Compound is abstract and Conformer its Implementation)
* Feature
* Dataset, FeatureValue, DataEntry
* Algorithm
* Parameter
* Model
* Task
* Token
* Policy
* Person/Group (for policies)
* Error Report
* BibTeX

Note: The API of ToxOtis will follow the structure described by the OpenTox ontology, so for each OpenTox entity (Dataset, Model, etc), as described in the OT ontology, there is a counterpart Java class in ToxOtis. This will facilitate the parsing of RDF documents coming from OpenTox services and will also lead to maximum cooperation, flexibility and extensibility in case of potent OT API amendments.

Ontologies:
* OpenTox ontology
* Endpoint ontology
* Feature Ontology
* Algorithm ontology
* External Ontologies (e.g. BibTex)

Ontology-related properties:
* DC Meta data (Dublin Core)
* OpenTox meta data (hasSource etc)

Clients:
* Get Client (Accept)
* Post (Configurable Headers, Parameter to be posted, Binary Data)
* Put
* Delete
* Options

Notes:
URL query parameters will be widely used in clients (should be seperate classes)
All operations will be combined with auth/auth API of OpenTox


h3. 1.2. Attributes for each component

Every component is uniquely identified by its URI (Consider using java.net.URI for that purpose). Also every component must have a set of meta-information describing it (DC and OT). Where necessary there should be a link to a BibTeX resource.
Each component needs a method to be transformed into an Ontological Model (OntModel) and of course a method for the inverse process (Parsing a given Ontological Model into a component). These methods can be bundled in an Interface called RDFable: <tt>OntModel createOntModel()</tt> and <tt>void createFromRDF(OntModel)</tt>.

h4. 1.2.1. Compound/Conformer:

* Chemical Identifiers: CasRN, Smiles


h4. 1.2.2. Feature

* Units of the feature
* Reference to BibTeX


h4. 1.2.3. Dataset

* A list of DataEntry objects

A dataset object holds as a private field the ontological model that describes it and has methods for iterating over its dataentries, compounds and values. 


h4. 1.2.4. DataEntry

* Compound
* Feature Values (Instances of FeatureValue) for the above compound


h4. 1.2.5. Algorithm

* Set of parameters (see Parameter)


h4. 1.2.6. Parameter

* Type of the Parameter (XSDDatatype)
* Value of the parameter (either used as its value or its default value)
* Scope (Optional/Mandatory)

h4. 1.2.7. Model

* Set of model parameters
* Predicted Feature
* Dependent Feature
* Set of independent features
* Training Dataset
* Algorithm used to produce the model
* BibTeX reference

h4. 1.2.8. Task

* 

h2. 2. Supported Functionalities

h3. 2.1. Related to compounds:

* Download a specific compound (given its URI)
* Search for compounds given their smiles etc (Use some search factory)
* Create new compound and get its URI (POSTing a compound to a dataset service)
* Update/Delete
* Get a representation of a compound and store it in a file
* Image? Get the depiction or the image from the URI
* Create a dataset with all available/some features for a compound
* Create/update feature for compound
* Delete a feature from the compound
* Get all conformers (list of Conformer objects)
* Remove/Update/Create conformer

Note: Make compound an ABSTRACT class, and Conformer its implementation


h3. 2.2. Related to features:

* Download feature info from a given URI (Parsing)
* Create new feature providing information about it/ Update feature/ Delete from remote URI
* Search for features given some search criterion (same as... etc)

h3. 2.3. Dataset
* Download and parse a dataset into a Dataset object
* Create a new dataset from scratch and either POST it or PUT in on a dataset server
* Create a dataset using alternative file formats like ARFF
* Create a dataset using online resources (compounds and features)  

h3. 2.4. Algorithm

* An enumeration of some available algorithms is needed (URIs)
* Create an Algorithm object providing the URI of an algorithm
* Apply an algorithm (might return a task or directly a model, dataset etc...)

Note: Tasks should be running on the background returning a Task object (see related section below).

h3. 2.5. Parameter

* Will be used as an auxiliary object (insie algorithms and models)

h3. 2.6. Model

* Apply a model on a dataset for prediction
* Apply a model on a certain compound for prediction
* Search for certain models (e.g. providing the dataset uri or the algorithm) on a Model Server
* Search for models using the ontology service

h3. 2.7. Task

* Given a URI of a task, download and parse it, check the status of the task
* Get a list of all tasks on a server 

